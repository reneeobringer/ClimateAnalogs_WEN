knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '/Users/rqo5125/Library/Mobile Documents/com~apple~CloudDocs/Documents/Teaching/data/EME597/FiveCitiesPMData')
library(neuralnet)
library(keras)
library(ISLR2)
xdata <- data.matrix(NYSE[, c("DJ_return", "log_volume","log_volatility")] )
istrain <- NYSE[, "train"]
xdata <- scale(xdata)
# function for time series lags
lagm <- function(x, k = 1) {
n <- nrow(x)
pad <- matrix(NA, k, ncol(x))
rbind(pad, x[1:(n - k), ])
}
# function for time series lags
lagm <- function(x, k = 1) {
n <- nrow(x)
pad <- matrix(NA, k, ncol(x))
rbind(pad, x[1:(n - k), ])
}
arframe <- data.frame(log_volume = xdata[, "log_volume"],
L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
L5 = lagm(xdata, 5))
rnn_mod <- keras_model_sequential() %>%
layer_simple_rnn(units = 12, input_shape = list(5, 3), dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
rnn_mod %>% compile(optimizer = optimizer_rmsprop(), loss = "mse")
rnn_modfit <- rnn_mod %>% fit(xrnn[istrain,, ], arframe[istrain, "log_volume"], batch_size = 64, epochs = 200,
validation_data = list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"]) )
# reorganize data for RNN
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1]) > xrnn <- array(xrnn, c(n, 3, 5))
# reorganize data for RNN
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
rnn_modfit <- rnn_mod %>% fit(xrnn[istrain,, ], arframe[istrain, "log_volume"], batch_size = 64, epochs = 200,
validation_data = list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"]) )
rnn_modfit <- rnn_mod %>% fit(xrnn[istrain,, ], arframe[istrain, "log_volume"], batch_size = 64, epochs = 10,
validation_data = list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"]) )
ypred <- predict(rnn_mod, xrnn[!istrain,, ])
1 - mean((ypred - arframe[!istrain, "log_volume"])^2) / V0
View(ypred)
rnn_modfit <- rnn_mod %>% fit(xrnn[istrain,, ], arframe[istrain, "log_volume"], batch_size = 64, epochs = 10,
validation_data = list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"]) )
ypred <- predict(rnn_mod, xrnn[!istrain,, ])
1 - mean((ypred - arframe[!istrain, "log_volume"])^2) / V1
rnn_modfit <- rnn_mod %>% fit(xrnn[istrain,, ], arframe[istrain, "log_volume"], batch_size = 64, epochs = 10,
validation_data = list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"]) )
ypred <- predict(rnn_mod, xrnn[!istrain,, ])
plot(rnn_modfit)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '/Users/rqo5125/Library/Mobile Documents/com~apple~CloudDocs/Documents/Teaching/data/EME597')
library(readxl)
solarpotential <- read_excel('worldsolarpotential.xlsx')
View(solarpotential)
View(solarpotential)
names(solarpotential)
View(solarpotential)
library(readxl)
solarpotential <- read_excel('worldsolarpotential.xlsx')
solarpotential <- na.omit(solarpotential)
solarpotential <- solarpotential[,-c(1:3,6,11)]
View(solarpotential)
names(solarpotential)
View(solarpotential)
library(readxl)
solarpotential <- read_excel('worldsolarpotential.xlsx')
solarpotential <- solarpotential[,-c(1:3,6,11)]
names(solarpotential) <- c('Population','Area','HDI','GDP','GHI','PVcap','RuralElectric','Consumption')
solarpotential <- as.numeric(solarpotential)
View(solarpotential)
library(readxl)
solarpotential <- read_excel('worldsolarpotential.xlsx')
solarpotential <- solarpotential[,-c(1:3,6,11)]
names(solarpotential) <- c('Population','Area','HDI','GDP','GHI','PVcap','RuralElectric','Consumption')
solarpotential <- lapply(solarpotential, as.numeric)
View(solarpotential)
library(readxl)
solarpotential <- read_excel('worldsolarpotential.xlsx')
solarpotential <- solarpotential[,-c(1:3,6,11)]
names(solarpotential) <- c('Population','Area','HDI','GDP','GHI','PVcap','RuralElectric','Consumption')
solarpotential <- sapply(solarpotential, as.numeric)
library(readxl)
solarpotential <- read_excel('worldsolarpotential.xlsx')
solarpotential <- solarpotential[,-c(1:3,6,11)]
names(solarpotential) <- c('Population','Area','HDI','GDP','GHI','PVcap','RuralElectric','Consumption')
solarpotential[] <- sapply(solarpotential, as.numeric)
View(solarpotential)
library(readxl)
solarpotential <- read_excel('worldsolarpotential.xlsx')
solarpotential <- solarpotential[,-c(1:3,6,11)]
names(solarpotential) <- c('Population','Area','HDI','GDP','GHI','PVcap','RuralElectric','Consumption')
solarpotential[] <- sapply(solarpotential, as.numeric)
solarpotential <- na.omit(solarpotential)
View(solarpotential)
```{r}
library(readxl)
solarpotential <- read_excel('worldsolarpotential.xlsx')
solarpotential <- solarpotential[,-c(1:3,6,11)]
names(solarpotential) <- c('Population','Area','HDI','GDP','GHI','PVcap','RuralElectric','Consumption')
solarpotential[] <- sapply(solarpotential, as.numeric)
solarpotential <- na.omit(solarpotential)
library(e1071)
library(ggplot2)
# Split into training and test sets
set.seed(1234)
# Split into training and test sets
set.seed(1234)
sample_rows <- sample(nrow(solarpotential), size = 0.8*nrow(solarpotential), replace = F)
trainingdata <- solarpotential[sample_rows,]
testdata <- solarpotential[-sample_rows,]
# fit the SVM
svm_mod <- svm(PVap ~ ., data = trainingdata)
# Split into training and test sets
set.seed(1234)
sample_rows <- sample(nrow(solarpotential), size = 0.8*nrow(solarpotential), replace = F)
trainingdata <- solarpotential[sample_rows,]
testdata <- solarpotential[-sample_rows,]
# fit the SVM
svm_mod <- svm(PVcap ~ ., data = trainingdata)
# predict using the test set
ypred <- predict(svm_mod, newdata = testdata) # predict with test set
print(paste('Test RMSE: ', sqrt(mean((testdata$PVcap - ypred)^2)),' MWp', sep = ''))
# create new dataframe for ggplot
plotdata <- data.frame(actual = testdata$PVcap, predicted = ypred)
# plot the actual vs. predicted values
ggplot(plotdata) + geom_point(aes(x = actual, y = predicted)) + geom_abline(slope = 1, intercept = 0, color = 'red') + ylab('Predicted PM2.5 (ug/m^3)') + xlab('Actual PM2.5 (ug/m^3)')
# create grid search for tuning
tuning <- tune(svm, PVcap ~ ., data = trainingdata, ranges = list(gamma = c(0.01, 0.1, 0.25), cost = c(4, 8, 16), degree = c(3,4,5)), kernel = 'polynomial')
# store best mode
tuned_svm_mod <- tuning$best.model
# predict using the test set
ypred <- predict(tuned_svm_mod, newdata = pmdata_new_test) # predict with test set
# create grid search for tuning
tuning <- tune(svm, PVcap ~ ., data = trainingdata, ranges = list(gamma = c(0.01, 0.1, 0.25), cost = c(4, 8, 16), degree = c(3,4,5)), kernel = 'polynomial')
# store best mode
tuned_svm_mod <- tuning$best.model
# predict using the test set
ypred <- predict(tuned_svm_mod, newdata = testdata) # predict with test set
print(paste('Test RMSE: ', sqrt(mean((testdata$PVcap - ypred)^2)),' MWp', sep = ''))
View(trainingdata)
# scale data
scaleddata <- scale(solarpotential)
pvscaled <- as.data.frame(scaleddata)
scalevalues <-  attr(scaleddata, 'scaled:scale')
centervalues <- attr(scaleddata, 'scaled:center')
# Split into training and test sets
set.seed(1234)
sample_rows <- sample(nrow(pvscaled), size = 0.8*nrow(pvscaled), replace = F)
pvnorm_train <- pvscaled[sample_rows,]
pvnorm_test <- pvscaled[-sample_rows,]
# set up model architecture
nn_mod <- keras_model_sequential() %>%
layer_dense(units = 30, activation = 'relu') %>%
layer_dense(units = 10, activation = 'relu') %>%
layer_dense(units = 25, activation = 'softmax') %>%
layer_dense(units = 1)
nn_mod %>% compile(optimizer = 'adam', loss = 'mse',metrics = list('mse', 'mae'))
# fit the model
data <- as.matrix(pvnorm_train[,-6])
labels <- as.matrix(pvnorm_train[,6])
val_data <- as.matrix(pvnorm_test[,-6])
val_labels <- as.matrix(pvnorm_test[,6])
modfit <- nn_mod %>% fit(data, labels, epochs = 100, validation_data = list(val_data, val_labels))
# prediction
ypred <- nn_mod %>% predict(val_data)
# un-scale the data
ypred <- ypred * scalevalues[1] + centervalues[1]
ytest <- pvnorm_test$PVcap * scalevalues[1] + centervalues[1]
print(paste('Test RMSE: ', sqrt(mean((ytest - ypred)^2)),' MWp', sep = ''))
# plots
plot(modfit)
ggplot() +
geom_point(aes(x = ytest, y = ypred)) +
geom_abline(slope = 1, intercept = 0, linetype = 'dashed')
ypred
pvnorm_test
val_labels
# scale data
scaleddata <- scale(solarpotential)
pvscaled <- as.data.frame(scaleddata)
scalevalues <-  attr(scaleddata, 'scaled:scale')
centervalues <- attr(scaleddata, 'scaled:center')
# Split into training and test sets
set.seed(1234)
sample_rows <- sample(nrow(pvscaled), size = 0.8*nrow(pvscaled), replace = F)
pvscale_train <- pvscaled[sample_rows,]
pvscale_test <- pvscaled[-sample_rows,]
# set up model architecture
nn_mod <- keras_model_sequential() %>%
layer_dense(units = 30, activation = 'relu') %>%
layer_dense(units = 10, activation = 'relu') %>%
layer_dense(units = 25, activation = 'softmax') %>%
layer_dense(units = 1)
nn_mod %>% compile(optimizer = 'adam', loss = 'mse',metrics = list('mse', 'mae'))
# fit the model
data <- as.matrix(pvscale_train[,-6])
labels <- as.matrix(pvscale_train[,6])
val_data <- as.matrix(pvscale_test[,-6])
val_labels <- as.matrix(pvscale_test[,6])
modfit <- nn_mod %>% fit(data, labels, epochs = 100, validation_data = list(val_data, val_labels))
# prediction
ypred <- nn_mod %>% predict(val_data)
# un-scale the data
ypredunscale <- ypred * scalevalues[1] + centervalues[1]
ytestunscale <- pvnorm_test$PVcap * scalevalues[1] + centervalues[1]
print(paste('Test RMSE: ', sqrt(mean((ytestunscale - ypredunscale)^2)),' MWp', sep = ''))
# plots
plot(modfit)
ggplot() +
geom_point(aes(x = ytestunscale, y = ypredunscale)) +
geom_abline(slope = 1, intercept = 0, linetype = 'dashed')
ypred
library(keras)
# scale data
scaleddata <- scale(solarpotential)
pvscaled <- as.data.frame(scaleddata)
scalevalues <-  attr(scaleddata, 'scaled:scale')
centervalues <- attr(scaleddata, 'scaled:center')
# Split into training and test sets
set.seed(1234)
sample_rows <- sample(nrow(pvscaled), size = 0.8*nrow(pvscaled), replace = F)
pvscale_train <- pvscaled[sample_rows,]
pvscale_test <- pvscaled[-sample_rows,]
# set up model architecture
nn_mod <- keras_model_sequential() %>%
layer_dense(units = 30, activation = 'relu') %>%
layer_dense(units = 10, activation = 'relu') %>%
layer_dense(units = 25, activation = 'softmax') %>%
layer_dense(units = 1)
nn_mod %>% compile(optimizer = 'adam', loss = 'mse',metrics = list('mae'))
# fit the model
data <- as.matrix(pvscale_train[,-6])
labels <- as.matrix(pvscale_train[,6])
val_data <- as.matrix(pvscale_test[,-6])
val_labels <- as.matrix(pvscale_test[,6])
modfit <- nn_mod %>% fit(data, labels, epochs = 100, validation_data = list(val_data, val_labels))
# prediction
ypred <- nn_mod %>% predict(val_data)
# un-scale the data
ypredunscale <- ypred * scalevalues[6] + centervalues[6]
ytestunscale <- pvnorm_test$PVcap * scalevalues[6] + centervalues[6]
print(paste('Test RMSE: ', sqrt(mean((ytestunscale - ypredunscale)^2)),' MWp', sep = ''))
# plots
plot(modfit)
ggplot() +
geom_point(aes(x = ytestunscale, y = ypredunscale)) +
geom_abline(slope = 1, intercept = 0, linetype = 'dashed')
# libraries
library(dplyr)
library(stringr)
library(tidyr)
library(reshape2)
library(ggplot2)
library(cowplot)
library(scales)
library(hydroGOF) # for NRMSE calculations
library(lubridate)
# directories
datadir <- '/Users/rqo5125/Library/Mobile Documents/com~apple~CloudDocs/Documents/Research/2024_25/papers/RenewableEnergy/datapaper/datafiles'
figuredir <- '/Users/rqo5125/Library/Mobile Documents/com~apple~CloudDocs/Documents/Research/2024_25/papers/RenewableEnergy/datapaper/figures'
# libraries
library(readxl)
########## LOAD DATA #################
precipdata <- read_excel(paste(datadir, '/acpcp.agg.2011.2022.xlsx'))
########## LOAD DATA #################
precipdata <- read_excel(paste(datadir, '/acpcp.agg.2011.2022.xlsx', sep = ''))
View(precipdata)
# NARR data
convectprecipdata <- read_excel(paste(datadir, '/acpcp.agg.2011.2022.xlsx', sep = ''))  # convective precipitation accumulation (kg/m^2)
temppdata <- read_excel(paste(datadir, '/air.agg.2011.2022.xlsx', sep = ''))            # air temperature (K)
albedodata <- read_excel(paste(datadir, '/albedo.agg.2011.2022.xlsx', sep = ''))        # albedo (%)
precipdata <- read_excel(paste(datadir, '/apcp.agg.2011.2022.xlsx', sep = ''))          # precipitation amount (kg/m^2)
dewpointdata <- read_excel(paste(datadir, '/dpt.agg.2011.2022.xlsx', sep = ''))         # dew point temperature (K)
pottempdata <- read_excel(paste(datadir, '/pottmp.agg.2011.2022.xlsx', sep = ''))       # potential temperature (K)
rhdata <- read_excel(paste(datadir, '/rhum.agg.2011.2022.xlsx', sep = ''))              # relative humidity (%)
cloudcoverdata <- read_excel(paste(datadir, '/tcdc.agg.2011.2022.xlsx', sep = ''))      # total cloud cover (%)
uwinddata <- read_excel(paste(datadir, '/uwnd.agg.2011.2022.xlsx', sep = ''))           # u-component of wind (m/s)
vwinddata <- read_excel(paste(datadir, '/vwnd.agg.2011.2022.xlsx', sep = ''))           # v-component of wind (m/s)
# EIA data
plantdata <- read_excel(paste(datadir, '/eia_master_df.xlsx', sep = ''))
View(albedodata)
# merge all NARR datasets
datasets <- list(convectprecipdata, temppdata, albedodata, precipdata, dewpointdata, pottempdata, rhdata, cloudcoverdata, uwinddata, vwinddata)
View(datasets)
library(purrr)
View(albedodata)
narrdata <- purrr::reduce(.x = datasets, merge, by = c('plant_code', 'year', 'month'), all = T)
View(narrdata)
# write to csv
write.csv(narrdata, 'allnardata.csv')
log10(2010*4547611)+0.33
10^(log10(2010*4547611)+0.33)
10^(log10(2010*4547611)+0.33)/4547611
10^(log10(2010)+0.33)
10^(1.03*log10(4547611) -3.88)
10^((1.03*log10(4547611) - 3.88) + 0.33)
2030-2010
rm(list=ls())
options(scipen = 999)
# Libraries
library(dplyr)
library(ggplot2)
library(ggrepel)
library(stringr)
library(data.table)
library(rlist)
library(tidyverse)
# Main Path
path <- '/Users/rqo5125/Library/Mobile Documents/com~apple~CloudDocs/Documents/Research/GitHub/public/ClimateAnalogs_WEN/'
# Directories
datadir1 <- paste(path, 'UtilityData/rawdata', sep = '')
datadir2 <- paste(path, '/ClimateData', sep = '')
# OPTIONAL: create an output directory for any any non-rdata output files (e.g., csv, pdf, etc.)
outputdir <- '/Users/rqo5125/Library/Mobile Documents/com~apple~CloudDocs/Documents/Research/2024_25/papers/scalingWEN/outputdata'
########### LOAD DATA #############
setwd(datadir1)
filelist <- list.files(pattern = "*.csv", full.names = TRUE)
# load cities with both water and electricity data
cities <- list()
for (filename in filelist) {
raw_df <- fread(filename)
raw_df <- na.omit(raw_df)
cities <- list.append(cities, raw_df)
}
# initialize variables
column_name <- "year"      # Column name to subset by
years_list <- c(2007:2018) # Condition to subset rows
beta_wateruse <- numeric(length(years_list))
intercept_wateruse <- numeric(length(years_list))
CI95lower_wateruse <- numeric(length(years_list))
CI95upper_wateruse <- numeric(length(years_list))
R2_wateruse <- numeric(length(years_list))
beta_elecuse <- numeric(length(years_list))
R2_elecuse <- numeric(length(years_list))
intercept_elecuse <- numeric(length(years_list))
CI95lower_elecuse <- numeric(length(years_list))
CI95upper_elecuse <- numeric(length(years_list))
output_df <- data.frame()
output_vars <- c("year", "beta_wateruse", "beta_elecuse",
"intercept_wateruse", "intercept_elecuse",
"r2_wateruse", "r2_elecuse",
"rmse_wateruse", "rmse_elecuse",
"nrmse_wateruse", "nrmse_elecuse",
"CI95lower_wateruse", "CI95upper_wateruse",
"CI95lower_elecuse", "CI95upper_elecuse")
# Loop through the cities
my_df_list <- list()
for (condition in years_list) {
# Create an empty list to store the subset data frames
ann_list <- list()
# Loop through each data frame in the list and subset rows
for (i in 1:length(cities)) {
df <- cities[[i]]
subset_df <- df[df[[column_name]] == condition, ]
ann_list[[i]] <- subset_df
}
# Resulting list of subset data frames
ann_list_avg <- list()
my_df <- data.frame()
for (i in 1:length(ann_list)) {
if (nrow(ann_list[[i]]) > 0) {
ann_avg <- ann_list[[i]]%>%
group_by(year) %>%
summarize(wateruse = mean(wateruse),
elecuse = mean(elecuse),
wateruse_population = mean(wateruse_population),
elecuse_population = mean(elecuse_population))
cityname <- strsplit(filelist[i], "./")[[1]][[2]]
cityname <- strsplit(cityname, ".csv")[[1]]
ann_avg[, "cityname"] = cityname
my_df[nrow(my_df) + 1,names(ann_avg)] = ann_avg
}
}
my_df_list <- list.append(my_df_list, my_df)
}
for (i in 1:length(years_list)) {
condition <- years_list[i]
# Calculate natural logarithms
my_df <- my_df_list[[i]]
ln_wateruse_population <- log(my_df$wateruse_population)
ln_elecuse <- log(my_df$elecuse)
ln_wateruse <- log(my_df$wateruse)
ln_elecuse_population <- log(my_df$elecuse_population)
# Perform linear regression
scaling_model_wateruse <- lm(ln_wateruse ~ ln_wateruse_population)
scaling_model_elecuse <- lm(ln_elecuse ~ ln_elecuse_population)
# Extract beta values (coefficients)
beta_wateruse <- coef(scaling_model_wateruse)
beta_elecuse <- coef(scaling_model_elecuse)
# get confidence intervals
CI95lower_wateruse <- confint(scaling_model_wateruse)[2,1]
CI95upper_wateruse <- confint(scaling_model_wateruse)[2,2]
CI95lower_elecuse <- confint(scaling_model_elecuse)[2,1]
CI95upper_elecuse <- confint(scaling_model_elecuse)[2,2]
# The first element is the intercept, and the second is the slope (scaling exponent)
intercept_wateruse <- beta_wateruse[1]
scaling_exponent_wateruse <- beta_wateruse[2]
intercept_elecuse <- beta_elecuse[1]
scaling_exponent_elecuse <- beta_elecuse[2]
# Extract R-squared value
r_squared_wateruse <- summary(scaling_model_wateruse)$r.squared
r_squared_elecuse <- summary(scaling_model_elecuse)$r.squared
# calculate rmse
rmse_wateruse <- sqrt(mean(scaling_model_wateruse$residuals^2))
rmse_elecuse <- sqrt(mean(scaling_model_elecuse$residuals^2))
nrmse_wateruse <- rmse_wateruse / sd(ln_wateruse)
nrmse_elecuse <- rmse_elecuse / sd(ln_elecuse)
# store as a data frame
output_df[nrow(
output_df) + 1,output_vars] = c(condition, scaling_exponent_wateruse,
scaling_exponent_elecuse, intercept_wateruse,
intercept_elecuse, r_squared_wateruse,
r_squared_elecuse, rmse_wateruse, rmse_elecuse,
nrmse_wateruse, nrmse_elecuse,
CI95lower_wateruse, CI95upper_wateruse,
CI95lower_elecuse, CI95upper_elecuse)
}
beta_wateruse
View(cities)
cityname
filename
filenames
filelist
View(cities)
my_df_list
scaling_model_wateruse
View(scaling_model_elecuse)
scaling_model_wateruse$residuals
View(my_df)
which(my_df$cityname == 'phoenix')
scaling_model_wateruse$residuals[which(my_df$cityname == 'phoenix')]
e^(0.89*log(1757055) - 4.28 + 0.53)
exp(0.89*log(1757055) - 4.28 + 0.53)
my_df$wateruse[which(my_df$cityname == 'phoenix')]
(7923.142-8496.786)/7923.142
(8496.786-7923.142)/7923.142
(8496.786-7923.142)/7923.142*100
